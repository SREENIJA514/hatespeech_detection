import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt   
from matplotlib import style      
style.use("ggplot")

import nltk
from nltk.util import pr
from nltk.tokenize import word_tokenize      
from nltk.stem import WordNetLemmatizer      
from nltk.corpus import stopwords            

from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

import re
import string

import nltk
nltk.download('stopwords')

from sklearn.model_selection import GridSearchCV
import warnings

from sklearn.metrics import accuracy_score

from sklearn.svm import SVC

from sklearn.naive_bayes import MultinomialNB

from sklearn.neighbors import KNeighborsClassifier

data = pd.read_csv("/content/drive/MyDrive/labeled_data.csv")
data.head()

data.info()

data.describe()

data["labels"] = data["class"].map({0: "Hate Speech",
                                    1: "Offensive Language",
                                    2: "No Hate and Offensive"})
print(data.head())

print(data["tweet"].iloc[0],"\n")
print(data["tweet"].iloc[1],"\n")
print(data["tweet"].iloc[2],"\n")
print(data["tweet"].iloc[3],"\n")
print(data["tweet"].iloc[4],"\n")

data = data[["tweet", "labels"]]
data.head()


stopword = stopwords.words('english')

def clean(text):
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub(r"\@w+|\#",'',text)
    text = re.sub(r"[^\w\s]",'',text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    tweet_tokens = word_tokenize(text)
    filtered_tweets=[w for w in tweet_tokens if not w in stopword] #removing stopwords
    return " ".join(filtered_tweets)

nltk.download('punkt')
data.tweet=data['tweet'].apply(clean)

tweetData = data.drop_duplicates("tweet")

tweetData.info()

lemmatizer=WordNetLemmatizer()
def lemmatizing(data):
    tweet=[lemmatizer.lemmatize(word) for word in data]
    return data

nltk.download('wordnet')
tweetData['tweet']=tweetData['tweet'].apply(lambda x: lemmatizing(x))

print(tweetData["tweet"].iloc[0],"\n")
print(tweetData["tweet"].iloc[1],"\n")
print(tweetData["tweet"].iloc[2],"\n")
print(tweetData["tweet"].iloc[3],"\n")
print(tweetData["tweet"].iloc[4],"\n")

tweetData['labels'].value_counts()

fig = plt.figure(figsize=(5,5))
ax = sns.countplot(x='labels', data=tweetData)

ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha="right")
plt.tight_layout()
plt.show()

fig = plt.figure(figsize=(7,7))
colors = ('red', 'green', 'blue')
wp = {'linewidth':2, "edgecolor":'black'}
tags = tweetData['labels'].value_counts()
explode=(0.1,0.1,0.1)
tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='')
plt.title("Distribution of sentiments")

non_hate_tweets = tweetData[tweetData.labels=='No Hate and Offensive']
non_hate_tweets.head()

non_hate_tweets.value_counts()

text=''.join([word for word in non_hate_tweets['tweet']])
plt.figure(figsize=(20,15), facecolor='None')
wordcloud=WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Most frequent words in non hate tweets", fontsize=19)
plt.show()

vect=TfidfVectorizer(ngram_range=(1,2)).fit(tweetData['tweet'])

feature_names=vect.get_feature_names_out()
print("Number of features: {}\n", format(len(feature_names)))
print("First 200 features: \n", format(feature_names[:20]))

vect=TfidfVectorizer(ngram_range=(1,3)).fit(tweetData['tweet'])

feature_names=vect.get_feature_names_out()
print("Number of features: {}\n", format(len(feature_names)))
print("First 200 features: \n", format(feature_names[:20]))

X = tweetData['tweet']
Y = tweetData['labels']
X = vect.transform(X)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("Size of X_train", (X_train.shape))
print("Size of Y_train", (Y_train.shape))
print("Size of X_test", (X_test.shape))
print("Size of Y_test", (Y_test.shape))

*LOGISTIC* *REGRESSION*

logreg = LogisticRegression()
logreg.fit(X_train, Y_train) #loading x_train and y_train data on model
logreg_predict = logreg.predict(X_test) #predicting the value for test data
logreg_acc = accuracy_score(logreg_predict, Y_test)

print("Test accuracy: {:.2f}%".format(logreg_acc*100))

print(confusion_matrix(Y_test, logreg_predict))
print("\n")
print(classification_report(Y_test, logreg_predict))

style.use('classic')
cm = confusion_matrix(Y_test, logreg_predict, labels=logreg.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)
disp.plot()

warnings.filterwarnings('ignore')

param_grid = {'C':[100, 10, 1.0, 0.1, 0.01], 'solver' :['newton-cg', 'lbfgs','liblinear']}
grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5)
grid.fit(X_train, Y_train)
print("Best Cross validation score: {:.2f}".format(grid.best_score_))
print("Best parameters: ", grid.best_params_)

log_grid_pred = grid.predict(X_test)

log_grid_acc = accuracy_score(log_grid_pred, Y_test)
print("Test accuracy: {:.2f}%".format(log_grid_acc*100))

print(confusion_matrix(Y_test, log_grid_pred))
print("\n")
print(classification_report(Y_test, log_grid_pred))

style.use('classic')
cm = confusion_matrix(Y_test, log_grid_pred, labels=grid.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=grid.classes_)
disp.plot()

*DECISION TREE CLASSIFIER*

dtree = DecisionTreeClassifier()
dtree.fit(X_train, Y_train) 
dtree_predict = dtree.predict(X_test) 
dtree_acc = accuracy_score(dtree_predict, Y_test)

print("Test accuracy: {:.2f}%".format(dtree_acc*100))

print(confusion_matrix(Y_test, dtree_predict))
print("\n")
print(classification_report(Y_test, dtree_predict))

style.use('classic')
cm = confusion_matrix(Y_test, dtree_predict, labels=dtree.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dtree.classes_)
disp.plot()


accuracy_score(Y_test, dtree_predict)

sample1 = "Welcome to WISE"
sample1 = clean(sample1)

sample1

data1 = vect.transform([sample1]).toarray()

data1

dtree.predict(data1)

sample = " bad bitches is the only thing that i like "
sample = clean(sample)

sample

data2 = vect.transform([sample]).toarray()

data2

dtree.predict(data2)

*SVM*



svm_model = SVC(kernel='linear', C=1.0, random_state=42)

svm_model.fit(X_train, Y_train)


svm_predict = svm_model.predict(X_test)

svm_acc = accuracy_score(svm_predict, Y_test)

print("Test accuracy (SVM): {:.2f}%".format(svm_acc * 100))

print(confusion_matrix(Y_test, svm_predict))
print("\n")
print(classification_report(Y_test, svm_predict))


style.use('classic')
cm = confusion_matrix(Y_test, svm_predict, labels=svm_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_model.classes_)
disp.plot()

*NAIVE BAYES*

nb_model = MultinomialNB()

nb_model.fit(X_train, Y_train)

nb_predict = nb_model.predict(X_test)

nb_acc = accuracy_score(nb_predict, Y_test)

print("Test accuracy (Naive Bayes): {:.2f}%".format(nb_acc * 100))

print(confusion_matrix(Y_test, nb_predict))
print("\n")
print(classification_report(Y_test, nb_predict))


style.use('classic')
cm = confusion_matrix(Y_test, nb_predict, labels=nb_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb_model.classes_)
disp.plot()

*KNN*

knn_model = KNeighborsClassifier(n_neighbors=5)

knn_model.fit(X_train, Y_train)

knn_predict = knn_model.predict(X_test)

knn_acc = accuracy_score(knn_predict, Y_test)

print("Test accuracy (KNN): {:.2f}%".format(knn_acc * 100))

print(confusion_matrix(Y_test, knn_predict))
print("\n")
print(classification_report(Y_test, knn_predict))


style.use('classic')
cm = confusion_matrix(Y_test, knn_predict, labels=knn_model.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn_model.classes_)
disp.plot()
